{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "\n",
    "from src.constants import AOIS_TEST\n",
    "from src.data import UNOSAT_S1TS_Dataset\n",
    "from src.classification.model_factory import load_model\n",
    "from src.classification.trainer import S1TSDD_Trainer\n",
    "\n",
    "def extract_features(df, start, end, prefix=\"\"):\n",
    "\n",
    "    # columns are datetime -> can slice directly between two dates\n",
    "    df = df.loc[:, start:end]\n",
    "\n",
    "    # features\n",
    "    df_features = pd.DataFrame(index=df.index)\n",
    "    df_features[\"mean\"] = df.mean(axis=1)\n",
    "    df_features[\"std\"] = df.std(axis=1)\n",
    "    df_features[\"median\"] = df.median(axis=1)\n",
    "    df_features[\"min\"] = df.min(axis=1)\n",
    "    df_features[\"max\"] = df.max(axis=1)\n",
    "    df_features[\"skew\"] = df.skew(axis=1)\n",
    "    df_features[\"kurt\"] = df.kurt(axis=1)\n",
    "\n",
    "    # rename columns using band, prefix (eg pre/post/pre_3x3, ...)\n",
    "    df_vv = df_features.xs(\"VV\", level=\"band\")\n",
    "    df_vh = df_features.xs(\"VH\", level=\"band\")\n",
    "    df_vv.columns = [f\"VV_{prefix}_{col}\" for col in df_vv.columns]\n",
    "    df_vh.columns = [f\"VH_{prefix}_{col}\" for col in df_vh.columns]\n",
    "    return pd.concat([df_vv, df_vh], axis=1)\n",
    "\n",
    "cfg = OmegaConf.create(\n",
    "    dict(\n",
    "        aggregation_method=\"mean\",\n",
    "        model_name=\"random_forest\",\n",
    "        model_kwargs=dict(\n",
    "            n_estimators=100,\n",
    "            n_jobs=12,\n",
    "        ),\n",
    "        data=dict(\n",
    "            aois_test = AOIS_TEST,\n",
    "            damages_to_keep=[1,2,3],\n",
    "            extract_winds = ['3x3'], # ['1x1', '3x3', '5x5']\n",
    "            random_neg_labels=0.1,  # percentage of negative labels to add in training set (eg 0.1 for 10%)\n",
    "            time_periods_pos = dict(\n",
    "                pre= ('2021-04-01', '2021-11-01'),\n",
    "                post= ('2022-04-01', '2022-11-01')\n",
    "            ),\n",
    "            time_periods_neg = dict(\n",
    "                pre = ('2020-04-01', '2020-11-01'),\n",
    "                post = ('2021-04-01', '2021-11-01')\n",
    "            )\n",
    "        ),\n",
    "        seed=123,\n",
    "        run_name=None,\n",
    "    )\n",
    ")\n",
    "\n",
    "ds = UNOSAT_S1TS_Dataset(cfg.data, extract_features=extract_features)\n",
    "model = load_model(cfg)\n",
    "trainer = S1TSDD_Trainer(ds, model, aggregation=cfg.aggregation_method, seed=cfg.seed, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df_preds_cv(df_preds):\n",
    "    df_preds_ = []\n",
    "    for i, df in enumerate(df_preds):\n",
    "        df['fold'] = i+1\n",
    "        df_preds_.append(df)\n",
    "    return pd.concat(df_preds_)\n",
    "\n",
    "df_preds_cv = concat_df_preds_cv(trainer.df_preds_cv)\n",
    "df_preds_agg_cv = concat_df_preds_cv(trainer.df_preds_agg_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.classification.utils import compute_metrics\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_metrics_across_folds_for_various_thresholds(\n",
    "        df,\n",
    "        metrics=['recall', 'precision'],\n",
    "        thresholds=np.arange(0.5, 1, 0.05)\n",
    "):\n",
    "\n",
    "    d_metrics = defaultdict(list)\n",
    "    for t in thresholds:\n",
    "        d_metrics_folds = defaultdict(list)\n",
    "        for fold, grp in df.groupby('fold'):\n",
    "            y_true = grp['label']\n",
    "            y_preds = (grp['preds_proba'] >= t).astype(int)\n",
    "            scores = compute_metrics(y_true, y_preds, verbose=0)\n",
    "            for m in metrics:\n",
    "                d_metrics_folds[m].append(scores[m])\n",
    "        for m in d_metrics_folds:\n",
    "            d_metrics[m].append(np.mean(d_metrics_folds[m]))\n",
    "            d_metrics[m + \"_std\"].append(np.std(d_metrics_folds[m]))\n",
    "    return pd.DataFrame(d_metrics, index=thresholds)\n",
    "\n",
    "\n",
    "\n",
    "def plot_metrics_curves(df_preds, metrics = ['recall', 'precision', 'f1'], agg=False):\n",
    "\n",
    "    thresholds = np.arange(0.5, 1, 0.05)\n",
    "    df_metrics = compute_metrics_across_folds_for_various_thresholds(df_preds, metrics=metrics,thresholds=thresholds)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10,6))\n",
    "    for m in metrics:\n",
    "        ax.plot(thresholds, df_metrics[m], label=m)\n",
    "        ax.fill_between(thresholds, df_metrics[m]-2*df_metrics[m+\"_std\"], df_metrics[m]+2*df_metrics[m+\"_std\"], alpha=0.2, label=m+\"Â±2*std\")\n",
    "\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.legend(loc='lower left')\n",
    "    title = 'Precision vs Recall for different thresholds across 5 folds'\n",
    "    if agg:\n",
    "        title += f\" (aggregated)\"\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_curves(df_preds_agg_cv, metrics = ['recall', 'precision', 'f1'], agg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_and_test();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_agg = trainer.df_preds_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neg = df_preds_agg_cv[df_preds_agg_cv['label'] == 0].copy()\n",
    "# df_pos = df_preds_agg_cv[df_preds_agg_cv['label'] == 1].copy()\n",
    "\n",
    "df_neg = df_preds_agg[df_preds_agg['label'] == 0].copy()\n",
    "df_pos = df_preds_agg[df_preds_agg['label'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of false positive per threshold\n",
    "thresholds = np.arange(5,9.5,0.5)/10 # to overcome floating point precisions\n",
    "\n",
    "fps = [len(df_neg[df_neg['preds_proba'] >= t]) for t in thresholds]\n",
    "tps = [len(df_pos[df_pos['preds_proba'] >= t]) for t in thresholds]\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(2,1,figsize=(10,10))\n",
    "# Plot false positives\n",
    "axs[0].semilogy(thresholds, fps, label=\"False Positives\")\n",
    "axs[0].set_xlabel(\"Threshold\")\n",
    "axs[0].set_ylabel(\"False Positives\")\n",
    "axs[0].legend(loc='lower left')\n",
    "axs[0].set_title(\"Number of False Positives per threshold\")\n",
    "for t, fp in zip(thresholds, fps):\n",
    "    axs[0].text(t, fp, f'{fp} ({100*fp/len(df_neg):.2f}%)', ha='left', va='bottom')\n",
    "axs[0].grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot false negatives\n",
    "axs[1].semilogy(thresholds, tps, label=\"True Positives\")\n",
    "axs[1].set_xlabel(\"Threshold\")\n",
    "axs[1].set_ylabel(\"True Positives\")\n",
    "axs[1].legend(loc='lower left')\n",
    "axs[1].set_title(\"Number of True Positives per threshold\")\n",
    "# ad a text box at 0.5, 0.6, 0.7, 0.8, 0.9 to show the number of false negatives\n",
    "for t, tp in zip(thresholds, tps):\n",
    "    axs[1].text(t, tp, f'{tp} ({100*tp/len(df_pos):.2f}%)', ha='left', va='bottom')\n",
    "axs[1].grid(linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of preds_proba\n",
    "df_neg['preds_proba'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.sort_values('preds_proba', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*8.2*5 + 61.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_unosat_labels\n",
    "\n",
    "labels = load_unosat_labels('UKR1')\n",
    "labels[['geometry']].loc[[10951]].explore()\n",
    "# labels[['geometry']].loc[[22173, 22165, 22189, 22164, 22175]].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.time_series import plot_ts_from_id, plot_all_ts_from_id\n",
    "\n",
    "plot_all_ts_from_id('UKR1', 10951)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import aoi_to_city\n",
    "aoi_to_city('UKR15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_unosat_labels\n",
    "\n",
    "aois_test = [\"UKR6\", \"UKR7\", \"UKR8\", \"UKR10\", \"UKR12\", \"UKR14\"]\n",
    "labels = [1,2,3]\n",
    "df = load_unosat_labels(labels_to_keep=labels)\n",
    "grouped = df.groupby(['aoi', 'damage']).size().reset_index(name='counts')\n",
    "df_count = grouped.pivot(index='aoi', columns='damage', values='counts').fillna(0).astype(int)\n",
    "\n",
    "n_test = df_count.loc[aois_test].sum()\n",
    "n_train = df_count.drop(aois_test).sum()\n",
    "n_tot = df_count.sum()\n",
    "\n",
    "for d in labels:\n",
    "    print(f\"Damage {d}:\")\n",
    "    print(f\"  - Train: {n_train[d]} ({100*n_train[d]/n_tot[d]:.2f}%)\")\n",
    "    print(f\"  - Test: {n_test[d]} ({100*n_test[d]/n_tot[d]:.2f}%)\")\n",
    "print('Total')\n",
    "print(f\"  - Train: {n_train.sum()} ({100*n_train.sum()/n_tot.sum():.2f}%)\")\n",
    "print(f\"  - Test: {n_test.sum()} ({100*n_test.sum()/n_tot.sum():.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.utils import aoi_orbit_iterator\n",
    "\n",
    "labels = [1,2,3]\n",
    "df = load_unosat_labels(labels_to_keep=labels)\n",
    "count = 0\n",
    "for aoi, orbit in aoi_orbit_iterator():\n",
    "    n = df[df.aoi==aoi].shape[0]\n",
    "    print(aoi ,orbit, n)\n",
    "    count+=n\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load_unosat_labels(labels_to_keep=[1,2,3])\n",
    "labels.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.groupby('aoi').date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aois = [\"UKR6\", \"UKR7\", \"UKR8\", \"UKR12\", \"UKR15\", \"UKR16\"]\n",
    "from src.data.utils import aoi_to_city\n",
    "for aoi in aois:\n",
    "    print(aoi, aoi_to_city(aoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "dates = labels.date.value_counts().index\n",
    "values = labels.date.value_counts().values\n",
    "ax.bar(dates, values, width=2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('HDate of analysis')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from src.constants import PROCESSED_PATH\n",
    "from src.data.unosat import assign_bins_to_labels\n",
    "\n",
    "labels_fp = PROCESSED_PATH / \"unosat_labels.feather\"\n",
    "gdf = gpd.read_feather(labels_fp).reset_index(drop=True)\n",
    "gdf = assign_bins_to_labels(gdf)\n",
    "gdf = gdf[gdf.aoi=='UKR7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[(gdf.prev_damage!=-99)&(gdf.damage>gdf.prev_damage)][['geometry', 'prev_damage', 'damage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[(gdf.prev_damage!=-99)&(gdf.damage>gdf.prev_damage)][['geometry', 'prev_damage', 'damage']].explore('damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import PROCESSED_PATH\n",
    "\n",
    "folder = PROCESSED_PATH / 'stacked_ts' / '1x1'\n",
    "import pandas as pd\n",
    "pd.read_csv(folder / 'UKR1_orbit_43.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_unosat_labels\n",
    "from src.data.utils import aoi_orbit_iterator\n",
    "\n",
    "labels = load_unosat_labels(labels_to_keep=[1,2,3])\n",
    "aois_test = [\"UKR6\", \"UKR7\", \"UKR8\", \"UKR10\", \"UKR12\", \"UKR14\"]\n",
    "\n",
    "count_train = 0\n",
    "count_test = 0\n",
    "for aoi, orbit in aoi_orbit_iterator():\n",
    "    n = labels[labels.aoi==aoi].shape[0]\n",
    "    if aoi in aois_test:\n",
    "        count_test += n\n",
    "    else:\n",
    "        count_train += n\n",
    "count_test, count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[aoi_to_city(aoi) for aoi in aois_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test/(count_train+count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels.aoi.isin(aois_test)].shape[0]/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[~labels.aoi.isin(aois_test)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5518/42435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
